{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graduated Non-Convexity (GNC) for robust spatial perceptions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as ani\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we want to fit models or estimate parameters by casting the problem into a least squares optimization:\n",
    "\n",
    "$$\n",
    "\\min_{x\\in\\cal{X}} \\sum_{i=1}^{N} r^2(\\bm{y}_i,x)\n",
    "$$\n",
    "\n",
    "This works great, if we have no outliers, but suffers severly from outliers, if there are any.\n",
    "\n",
    "OK, to start and see we first generate some demo data (with additive noise) and plot them to understand what we have got. We use a line as a model, paramtereized by [a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng()\n",
    "\n",
    "# let g be a kinear function of x\n",
    "def linear_model(theta, x):\n",
    "    return (theta[0] * x + theta[1])\n",
    "\n",
    "# generate some inlier data\n",
    "x_data = np.linspace(-40,40)\n",
    "a=3; b=1; noise = 4\n",
    "y_data = linear_model([a,b],x_data) + noise * rng.standard_normal(x_data.shape[0])\n",
    "\n",
    "# generate some outliers\n",
    "\n",
    "noise2 = 80\n",
    "x2_data = np.linspace(-40,40,200)\n",
    "y2_data = noise2 * rng.standard_normal(x2_data.shape[0])\n",
    "\n",
    "print('outlier rate: ' +  repr(x2_data.size / (x_data.size + x2_data.size)))\n",
    "\n",
    "plt.scatter(x_data,y_data, s=4)\n",
    "plt.scatter(x2_data,y2_data, s=4)\n",
    "# plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we fit a line, with standard least squares, this is what we get: yellow is the line fit, red is ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimization to estimate the parameters, rho is the loss function\n",
    "\n",
    "def rho(theta,x,y,w):\n",
    "    return np.sqrt(w)*(linear_model(theta, x) - y)\n",
    "\n",
    "# do the actual optimization with both inliers and outliers\n",
    "x_all = np.hstack((x_data,x2_data))\n",
    "y_all = np.hstack((y_data,y2_data))\n",
    "weights = np.ones_like(x_all)\n",
    "theta0 = [0,1]\n",
    "\n",
    "result = optimize.least_squares(rho, theta0, args=(x_all,y_all,weights),verbose=0)\n",
    "plt.plot(x_data,linear_model(result.x,x_data), c = 'y')\n",
    "plt.plot(x_data,linear_model([a,b],x_data), c = 'r')\n",
    "plt.scatter(x_all,y_all, s=4)\n",
    "#plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A robust cost function $\\rho(\\cdot)$ limits the influence of outliers on the final estimation result. \n",
    "\n",
    "$$\n",
    "\\min_{x\\in\\cal{X}} \\sum_{i=1}^{N} \\rho(r^2(\\bm{y}_i,x))\n",
    "$$\n",
    "\n",
    "\n",
    "Here we Geman McClure (GM), where $\\bar{c}$ is a parameter that determinse the shape of the fuction, and $r$ is the residual\n",
    "\n",
    "$$\n",
    "\\rho(r) = \\frac{\\bar{c}^2 r^2}{\\bar{c}^2 + r^2}\n",
    "$$\n",
    "\n",
    "In addition, we add a parameter $\\mu$ that allows to adjust the cost function from convex ($\\mu \\rightarrow \\infty$) to the non-convex ($\\mu = 1$) function  $\\rho(\\cdot)$ given above.\n",
    "\n",
    "$$\n",
    "\\rho_{\\mu}(r) = \\frac{\\mu\\bar{c}^2 r^2}{\\mu\\bar{c}^2 + r^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rho_gm(c,r,mu):\n",
    "    return (mu*c**2*r**2) / (mu*c**2 + r**2)\n",
    "\n",
    "c_bar = 1\n",
    "r_data = np.linspace(-9,9,200)\n",
    "\n",
    "for mu in np.linspace(1,20,15):\n",
    "    plt.plot(r_data,rho_gm(c_bar,r_data,mu), c = 'g', alpha = np.sqrt(1/(21-mu)))\n",
    "\n",
    "plt.plot(r_data,rho_gm(c_bar,r_data,1), c = 'r')\n",
    "\n",
    "# plt.axis('equal')\n",
    "plt.ylim([0,9])\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's assume we want to do a robust fit with roboust cost function. We start with a more or less convex cost function, and in an iterative fashion we increase its non-convexity by decreasing $\\mu \\rightarrow 1$.\n",
    "\n",
    "For details see Yang et al., \"Graduated non-convexity for robust spatial perception: From non-minimal solvers to global outlier rejection.\" IEEE Robotics and Automation Letters 5.2 (2020): 1127-1134."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimization to estimate the parameters, rho is the loss function\n",
    "def rho(theta,x,y,w):\n",
    "    return np.sqrt(w)*(linear_model(theta, x) - y)\n",
    "\n",
    "# setup a figure with two subplots one for the data and one for the rho function\n",
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "fig.set_size_inches(6,2.5)\n",
    "ax2.set_ylim([0,200])\n",
    "\n",
    "# setup file writer for animatd GIF\n",
    "metadata = dict(title='Movie Test', artist='Matplotlib',\n",
    "                comment='Movie support!')\n",
    "writer = ani.PillowWriter(fps=8, metadata=metadata)\n",
    "\n",
    "# initialize the actual optimization with both inliers and outliers\n",
    "x_all = np.hstack((x_data,x2_data))\n",
    "y_all = np.hstack((y_data,y2_data))\n",
    "weights = np.ones_like(x_all)\n",
    "theta0 = [-1,-1]\n",
    "\n",
    "\n",
    "# the wheight update according to Yang et al.\n",
    "def weights_update(resid,c_, mu):\n",
    "    return (mu*c_*c_ / (resid**2+mu*c_*c_))**2\n",
    "\n",
    "# plot start value in yellow\n",
    "ax1.plot(x_data,linear_model(theta0,x_data), c = 'b')\n",
    "\n",
    "# first iteration (will result into naive fiting  including all outliers) to compute initial residuals needed for first wheight update\n",
    "result = optimize.least_squares(rho, theta0, args=(x_all,y_all,weights),verbose=0)\n",
    "residuals = linear_model(result.x, x_all) - y_all\n",
    "ax1.plot(x_data,linear_model(result.x,x_data), c = 'y')\n",
    "\n",
    "# create to be animated green line, initially identical to the yellow result of the first iteration\n",
    "current_fit = ax1.plot(x_data,linear_model(result.x,x_data), c = 'g', linewidth=2)[0]\n",
    "\n",
    "# set the noise threshold according to Yang et al.\n",
    "from scipy.stats.distributions import chi2\n",
    "c_bar = np.sqrt(chi2.ppf(0.8, df=2)*noise)\n",
    "\n",
    "# initialize mu\n",
    "mu = mu_start = 2 * (np.max(residuals)**2)/(c_bar**2)\n",
    "\n",
    "#perform first wheights update \n",
    "weights = weights_update(residuals,c_bar, mu)\n",
    "\n",
    "size = 90 * weights\n",
    "scat = ax1.scatter(x_all, y_all, s=size, alpha=0.5)\n",
    "\n",
    "\n",
    "iter_count = 0\n",
    "\n",
    "with writer.saving(fig, \"gnc_demo.gif\", fig.dpi):\n",
    "    while (mu > 1):\n",
    "        alpha = ((iter_count / 50)) #**2\n",
    "\n",
    "        ax1.plot(x_data,linear_model(theta0,x_data), c = 'grey', alpha = alpha, linewidth=1)\n",
    "        ax2.plot(x_data,rho_gm(c_bar,x_data,mu), c = 'grey', alpha = alpha, linewidth=1)\n",
    "        \n",
    "        # one iteration\n",
    "        result = optimize.least_squares(rho, theta0, args=(x_all,y_all,weights),verbose=0)\n",
    "        theta0 = result.x \n",
    "        residuals = linear_model(result.x, x_all) - y_all\n",
    "        weights = weights_update(residuals,c_bar, mu)\n",
    "        mu = mu / 1.2\n",
    "        iter_count = iter_count+1\n",
    "        \n",
    "        # update plot for animation\n",
    "        size = 90 * weights\n",
    "        #size[size < 0.1] = 0.1\n",
    "        scat.set_sizes(size)\n",
    "        current_fit.set_data(x_data, linear_model(result.x, x_data))\n",
    "        \n",
    "        writer.grab_frame()\n",
    "\n",
    "    # plot the final results\n",
    "    ax1.plot(x_data,linear_model([a,b],x_data), c = 'r', linewidth=3)\n",
    "    ax1.plot(x_data,linear_model(result.x,x_data), c = 'g', linewidth=2)\n",
    "\n",
    "    ax2.plot(x_data,rho_gm(c_bar,x_data,1), c = 'g', linewidth=2)\n",
    "    \n",
    "\n",
    "    for i in range(20):\n",
    "        writer.grab_frame() \n",
    "\n",
    "print(theta0)\n",
    "\n",
    "\n",
    "plt.figure(3)\n",
    "x_data3 = np.linspace(-40,40,200)\n",
    "\n",
    "plt.plot(x_data3,rho_gm(c_bar,x_data3,1), c = 'g', linewidth=2)\n",
    "plt.ylim([0,20])\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
