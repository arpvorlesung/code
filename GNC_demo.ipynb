{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graduated Non-Convexity (GNC) for robust spatial perceptions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi'] = 150\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we want to fit models or estimate parameters by casting the problem into a least squares optimization:\n",
    "\n",
    "$$\n",
    "\\min_{x\\in\\cal{X}} \\sum_{i=1}^{N} r^2(\\bm{y}_i,x)\n",
    "$$\n",
    "\n",
    "This works great, if we have no outliers, but suffers severly from outliers, if there are any.\n",
    "\n",
    "OK, to start and see we first generate some demo data (with additive noise) and plot them to understand what we have got. We use a line as a model, paramtereized by [a,b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng()\n",
    "\n",
    "# let g be a kinear function of x\n",
    "def linear_model(theta, x):\n",
    "    return (theta[0] * x + theta[1])\n",
    "\n",
    "# generate some inlier data\n",
    "x_data = np.linspace(-40,40)\n",
    "a=3; b=1; noise = 4\n",
    "y_data = linear_model([a,b],x_data) + noise * rng.standard_normal(x_data.shape[0])\n",
    "\n",
    "# generate some outliers\n",
    "\n",
    "noise2 = 80\n",
    "x2_data = np.linspace(-40,40,200)\n",
    "y2_data = noise2 * rng.standard_normal(x2_data.shape[0])\n",
    "\n",
    "print('outlier rate: ' +  repr(x2_data.size / (x_data.size + x2_data.size)))\n",
    "\n",
    "plt.scatter(x_data,y_data, s=4)\n",
    "plt.scatter(x2_data,y2_data, s=4)\n",
    "# plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we fit a line, with standard least squares, this is what we get: yellow is the line fit, red is ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimization to estimate the parameters, rho is the loss function\n",
    "\n",
    "def rho(theta,x,y,w):\n",
    "    return np.sqrt(w)*(linear_model(theta, x) - y)\n",
    "\n",
    "# do the actual optimization with both inliers and outliers\n",
    "x_all = np.hstack((x_data,x2_data))\n",
    "y_all = np.hstack((y_data,y2_data))\n",
    "weights = np.ones_like(x_all)\n",
    "theta0 = [0,1]\n",
    "\n",
    "result = optimize.least_squares(rho, theta0, args=(x_all,y_all,weights),verbose=0)\n",
    "plt.plot(x_data,linear_model(result.x,x_data), c = 'y')\n",
    "plt.plot(x_data,linear_model([a,b],x_data), c = 'r')\n",
    "plt.scatter(x_all,y_all, s=4)\n",
    "#plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A robust cost function $\\rho(\\cdot)$ limits the influence of outliers on the final estimation result. \n",
    "\n",
    "$$\n",
    "\\min_{x\\in\\cal{X}} \\sum_{i=1}^{N} \\rho(r^2(\\bm{y}_i,x))\n",
    "$$\n",
    "\n",
    "\n",
    "Here we Geman McClure (GM), where $\\bar{c}$ is a parameter that determinse the shape of the fuction, and $r$ is the residual\n",
    "\n",
    "$$\n",
    "\\rho(r) = \\frac{\\bar{c}^2 r^2}{\\bar{c}^2 + r^2}\n",
    "$$\n",
    "\n",
    "In addition, we add a parameter $\\mu$ that allows to adjust the cost function from convex ($\\mu \\rightarrow \\infty$) to the non-convex ($\\mu = 1$) function  $\\rho(\\cdot)$ given above.\n",
    "\n",
    "$$\n",
    "\\rho_{\\mu}(r) = \\frac{\\mu\\bar{c}^2 r^2}{\\mu\\bar{c}^2 + r^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rho_gm(c,r,mu):\n",
    "    return (mu*c**2*r**2) / (mu*c**2 + r**2)\n",
    "\n",
    "c_bar = 1\n",
    "r_data = np.linspace(-9,9,200)\n",
    "\n",
    "for mu in np.linspace(1,20,15):\n",
    "    plt.plot(r_data,rho_gm(c_bar,r_data,mu), c = 'g', alpha = np.sqrt(1/(21-mu)))\n",
    "\n",
    "plt.plot(r_data,rho_gm(c_bar,r_data,1), c = 'r')\n",
    "\n",
    "# plt.axis('equal')\n",
    "plt.ylim([0,9])\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's assume we want to do a robust fit with roboust cost function. \n",
    "\n",
    "For details see Yang et al., \"Graduated non-convexity for robust spatial perception: From non-minimal solvers to global outlier rejection.\" IEEE Robotics and Automation Letters 5.2 (2020): 1127-1134."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup optimization to estimate the parameters, rho is the loss function\n",
    "\n",
    "def rho(theta,x,y,w):\n",
    "    return np.sqrt(w)*(linear_model(theta, x) - y)\n",
    "\n",
    "# do the actual optimization with both inliers and outliers\n",
    "x_all = np.hstack((x_data,x2_data))\n",
    "y_all = np.hstack((y_data,y2_data))\n",
    "weights = np.ones_like(x_all)\n",
    "theta0 = [-1,-1]\n",
    "\n",
    "def weights_update(resid,c_, mu):\n",
    "    return (mu*c_*c_ / (resid**2+mu*c_*c_))**2\n",
    "\n",
    "plt.figure(1); plt.plot(x_data,linear_model(theta0,x_data), c = 'b')\n",
    "\n",
    "result = optimize.least_squares(rho, theta0, args=(x_all,y_all,weights),verbose=0)\n",
    "residuals = linear_model(result.x, x_all) - y_all\n",
    "plt.figure(1); plt.plot(x_data,linear_model(result.x,x_data), c = 'y')\n",
    "\n",
    "# set the noiuse threshold\n",
    "from scipy.stats.distributions import chi2\n",
    "c_bar = np.sqrt(chi2.ppf(0.8, df=2)*noise)\n",
    "\n",
    "mu = mu_start = 2 * (np.max(residuals)**2)/(c_bar**2)\n",
    "weights = weights_update(residuals,c_bar, mu)\n",
    "iter_count = 0\n",
    "\n",
    "while (mu > 1):\n",
    "    alpha = ((iter_count / 50))**2\n",
    "    plt.figure(1); plt.plot(x_data,linear_model(theta0,x_data), c = 'grey', alpha = alpha, linewidth=1)\n",
    "    plt.figure(2); plt.plot(x_data,rho_gm(c_bar,x_data,mu), c = 'grey', alpha = alpha, linewidth=1)\n",
    "    result = optimize.least_squares(rho, theta0, args=(x_all,y_all,weights),verbose=0)\n",
    "    theta0 = result.x \n",
    "    residuals = linear_model(result.x, x_all) - y_all\n",
    "    weights = weights_update(residuals,c_bar, mu)\n",
    "    mu = mu / 1.2\n",
    "    iter_count = iter_count+1\n",
    "\n",
    "print(theta0)\n",
    "\n",
    "# plot the results\n",
    "\n",
    "plt.figure(1)\n",
    "size = 90 * weights\n",
    "size[size < 1] = 1\n",
    "plt.scatter(x_all, y_all, s=size, alpha=0.5)\n",
    "plt.plot(x_data,linear_model([a,b],x_data), c = 'r', linewidth=3)\n",
    "plt.plot(x_data,linear_model(result.x,x_data), c = 'g', linewidth=2)\n",
    "\n",
    "# plt.axis('equal')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(x_data,rho_gm(c_bar,x_data,1), c = 'g', linewidth=2)\n",
    "plt.ylim([0,600])\n",
    "\n",
    "plt.figure(3)\n",
    "x_data3 = np.linspace(-40,40,200)\n",
    "\n",
    "plt.plot(x_data3,rho_gm(c_bar,x_data3,1), c = 'g', linewidth=2)\n",
    "plt.ylim([0,20])\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
